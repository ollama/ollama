# Fix Critical Data Loss in mxbai-embed-large Model (80% Batch Failure)

## Issue
The `mxbai-embed-large` embedding model was experiencing catastrophic data loss where 80% of batch requests were failing (5000 inputs â†’ ~1000 outputs) due to incorrect batch processing using `llama_decode()` instead of `llama_encode()` for encoder-only models.

### Root Cause
- **Architecture mismatch**: Using decoder API (`llama_decode()`) for encoder-only models
- **Error pattern**: "cannot decode batches with this context (use llama_encode() instead)"
- **Impact**: Production systems losing 80% of embedding data

## Solution
Implemented smart routing based on model architecture detection:

1. **Auto-detection**: Added `HasEncoder()` and `HasDecoder()` functions to identify model types
2. **Smart routing**: Encoder-only models automatically routed to `llama_encode()`, decoder models use `llama_decode()`
3. **Enhanced validation**: Added comprehensive error checking and data integrity validation
4. **Thread safety**: Implemented defensive copying to prevent race conditions

## Code Changes

### llama/llama.go
```go
// Architecture detection functions
func (m *Model) HasEncoder() bool {
    return bool(C.llama_model_has_encoder(m.c))
}

func (m *Model) HasDecoder() bool {
    return bool(C.llama_model_has_decoder(m.c))
}

// Smart routing in Decode function
func (c *Context) Decode(batch *Batch) error {
    // Auto-detect encoder-only models and route to appropriate function
    if c.Model().HasEncoder() && !c.Model().HasDecoder() {
        return c.Encode(batch)
    }
    // ... existing decoder logic
}

// New Encode function for encoder-only models
func (c *Context) Encode(batch *Batch) error {
    code := int(C.llama_encode(c.c, batch.c))
    if code < 0 {
        return fmt.Errorf("llama_encode failed with code %d", code)
    }
    return nil
}

// Enhanced GetEmbeddings with validation
func (c *Context) GetEmbeddingsSeq(seqId int) []float32 {
    // Added dimension validation, copy validation, and NaN/Inf detection
}
```

### server/routes.go
```go
// Enhanced batch processing with validation
for i, text := range input {
    i := i // capture loop variable
    g.Go(func() error {
        embedding, err := r.Embedding(c.Request.Context(), text)
        if err != nil {
            return err
        }
        
        // Multi-level validation
        if embedding == nil {
            return fmt.Errorf("embedding generation returned nil for input index %d", i)
        }
        if len(embedding) == 0 {
            return fmt.Errorf("embedding generation returned empty vector for input index %d", i)
        }
        
        // Create defensive copy to prevent race conditions
        embeddingCopy := make([]float32, len(embedding))
        copy(embeddingCopy, embedding)
        
        normalized := normalize(embeddingCopy)
        if normalized == nil {
            return fmt.Errorf("normalization failed for input index %d", i)
        }
        
        embeddings[i] = normalized
        return nil
    })
}

// Enhanced normalization with mathematical validation
func normalize(vec []float32) []float32 {
    if vec == nil {
        return nil
    }
    
    // ... normalization logic with NaN/Inf validation
    if math.IsNaN(float64(norm)) || math.IsInf(float64(norm), 0) {
        return nil
    }
    // ...
}
```

### runner/llamarunner/runner.go
```go
// Enhanced embedding validation
if embed == nil {
    seq.embedding <- nil
    s.removeSequence(i, llm.DoneReasonStop)
    continue
}

if len(embed) == 0 {
    seq.embedding <- nil
    s.removeSequence(i, llm.DoneReasonStop)
    continue
}

// Create defensive copy to prevent race conditions
embedCopy := make([]float32, len(embed))
copy(embedCopy, embed)

seq.embedding <- embedCopy
```

## Testing

### Production Stress Test Results
```
PRODUCTION STRESS TEST: mxbai-embed-large Data Loss Fix
Testing 5000 inputs with mxbai-embed-large (original issue scenario)

Expected: 5000 embeddings
Received: 5000 embeddings
SUCCESS: All embeddings generated successfully!
The catastrophic 80% data loss issue has been RESOLVED!

Performance Metrics:
Average time per embedding: 64.95ms
Processing throughput: 15.40 embeddings/second
```

### Test Coverage
- **Integration tests**: End-to-end API validation with real models
- **Stress tests**: Large batch processing (5000 inputs)
- **Consistency tests**: Single vs batch API alignment
- **Data integrity tests**: NaN/Inf detection and corruption prevention

## Results

### Before Fix
```
Input: 5000 text samples
Output: ~1000 embeddings (80% data loss)
Errors: "cannot decode batches with this context" repeated
Status: CRITICAL PRODUCTION FAILURE
```

### After Fix
```
Input: 5000 text samples
Output: 5000 embeddings (100% success rate)
Errors: None
Performance: ~65ms per embedding (consistent)
Status: FULLY FUNCTIONAL - PRODUCTION READY
```

## Backward Compatibility
- **Zero breaking changes**: All existing APIs work unchanged
- **Model compatibility**: All existing models continue to function
- **Performance**: No regression for decoder models
- **Deployment**: Drop-in replacement for existing installations

## How to Test
```bash
# Build the fixed version
go build -o ollama .

# Pull the test model
./ollama pull mxbai-embed-large

# Start the server
./ollama serve &

# Run production stress test (5000 inputs)
python3 test_large_batch_stress.py

# Expected: 5000 embeddings generated successfully (100% success rate)
```

## Conclusion
This fix resolves the critical 80% data loss issue in mxbai-embed-large batch processing by implementing smart routing based on model architecture detection. The solution is production-ready, maintains full backward compatibility, and eliminates the catastrophic data loss affecting production systems using encoder-only embedding models.

**Status**: COMPLETE - Ready for production deployment
